# Epic 4: Convergence Engine

**Duration:** 7 days  
**Token Budget:** 50K tokens  
**Status:** Not Started  
**Dependencies:** Epic 3.75 (Code Execution Layer)  
**Blocks:** Epic 5 (Multi-Agent Orchestration)

---

## Epic Goal

Build the Answer Contract-driven convergence engine that iteratively generates code until validation passes, using Anthropic's code-first pattern for 98% token reduction, with adaptive iteration limits, timeout handling, and deterministic evidence packs for SOC 2 compliance.

---

## Context: The Convergence Problem

### Traditional Approach (Fails)
- ‚ùå One-shot code generation (70% failure rate)
- ‚ùå No validation feedback loop
- ‚ùå No quality improvement iteration
- ‚ùå Manual verification required

### FORGE Convergence Solution
- ‚úÖ **Iterative refinement**: Generate ‚Üí Validate ‚Üí Refine ‚Üí Repeat
- ‚úÖ **Contract-driven**: Answer Contract defines "done"
- ‚úÖ **Automated validation**: Code-first validators (deterministic)
- ‚úÖ **Evidence-based**: Every convergence produces audit trail
- ‚úÖ **Token-efficient**: 98% reduction via code-first execution

---

## Anthropic Guidance Applied (from EPIC-4-CONVERGENCE-QUESTIONS-ANTHROPIC-GUIDANCE.md)

### Question 1: Should convergence default to code-first for all internal validators?
**‚úÖ YES (100% confidence)**

**Rationale:**
- Anthropic explicitly recommends code-first (98.7% token reduction)
- Production validated (GitHub MCP Server: 99%+ reduction with 112 tools)
- "LLMs are adept at writing code - leverage this strength"

**Implementation:**
```typescript
// Code-first validators (recommended)
const validationResult = await executeCode(`
  const validators = contract.validators || [];
  const results = validators.map(v => runValidator(v, code));
  return results;
`);

// vs Traditional MCP (avoid)
for (const validator of validators) {
  await tools.validate({ validator }); // Each call consumes context
}
```

### Question 2: What's the appropriate max_iterations default?
**‚úÖ 15 iterations (95% confidence)**

**Rationale:**
- Anthropic's agents use **10-15 task limits** before compaction
- "Claude's outputs improve after **2-3 iterations**"
- Fits 50K token epic budget (15 √ó 3,500 = 52.5K)
- Allows **2-3 improvement rounds per component**

**Configuration:**
```typescript
const CONVERGENCE_CONFIG = {
  maxIterations: 15,              // Anthropic guidance: 10-15
  stagnationLimit: 3,              // Stop if no improvement for 3 iterations
  minScore: 0.85,                  // Early stop threshold
  tokenBudget: 50000,              // Epic token limit
  iterationBudget: 3500,           // Tokens per iteration
};
```

### Question 3: How should we handle convergence timeout?
**‚úÖ Async execution with circuit breakers (100% confidence)**

**Rationale:**
- Anthropic's async pattern: "Continue other work while waiting"
- MCP timeout: 30 seconds recommended
- Code execution: Configurable (30s-120s)
- Graceful degradation on timeout

**Timeout Strategy:**
```typescript
export interface TimeoutStrategy {
  quickValidation: 30000,        // 30s - Syntax, lint, type checks
  standardIteration: 60000,      // 60s - Code generation + validation
  complexIteration: 120000,      // 120s - Integration tests, E2E
  absoluteMax: 180000,           // 180s - Before forced degradation
}
```

### Question 4: Should evidence pack generation happen in code or via LLM?
**‚úÖ Hybrid: Code for collection, LLM for synthesis (100% confidence)**

**Rationale:**
- **Code-first collection**: Deterministic, auditable, SOC 2 compliant
- **LLM synthesis**: Human-readable summaries and insights
- Anthropic SOC 2 guidance: "Validate outputs, track model versions"

**Evidence Pack Structure:**
```typescript
export interface EvidencePack {
  // ‚úÖ CODE-GENERATED (Deterministic)
  metadata: { runId, timestamp, epic, contract, agent };
  rawEvidence: { toolInvocations, validationResults, artifacts, metrics };
  integrity: { sha256, signature, timestamp };
  
  // ‚ùå LLM-GENERATED (Non-Deterministic)
  synthesis: { executiveSummary, keyFindings, recommendations, narrative };
}
```

---

## User Stories

### US-4.1: Convergence Loop Engine
**As a** convergence engine  
**I want** to iteratively generate and validate code  
**So that** quality improves with each iteration

**Acceptance Criteria:**
- [ ] Load Answer Contract from YAML
- [ ] Initialize convergence state (iteration 0)
- [ ] Execute convergence loop (generate ‚Üí validate ‚Üí refine)
- [ ] Stop conditions (converged, max iterations, stagnation)
- [ ] Return convergence result with evidence pack

**Convergence Loop:**
```typescript
async function converge(contract: AnswerContract): Promise<ConvergenceResult> {
  let state = initializeState(contract);
  
  while (!shouldStop(state)) {
    // Generate code (LLM)
    const code = await generateCode(state);
    
    // Validate code (code-first, deterministic)
    const validation = await validateCode(code, contract.validators);
    
    // Update state
    state = updateState(state, code, validation);
    
    // Check convergence
    if (hasConverged(state, contract.threshold)) {
      break;
    }
  }
  
  return {
    converged: state.score >= contract.threshold,
    finalCode: state.code,
    iterations: state.iterations.length,
    evidencePack: generateEvidencePack(state),
  };
}
```

---

### US-4.2: Code-First Validator Execution
**As a** convergence engine  
**I want** validators to run as TypeScript code in Deno sandbox  
**So that** validation is deterministic and token-efficient

**Acceptance Criteria:**
- [ ] Parse validator definitions from Answer Contract
- [ ] Generate TypeScript validator code
- [ ] Execute in Deno sandbox (30s timeout)
- [ ] Return structured validation results

**Validator Types:**
```typescript
export type ValidatorType = 
  | 'syntax'        // ESLint, TypeScript compiler
  | 'unit_test'     // Jest, Vitest
  | 'type_check'    // TypeScript type checker
  | 'security'      // npm audit, Snyk
  | 'performance'   // Bundle size, execution time
  | 'custom';       // User-defined validators
```

---

### US-4.3: Adaptive Iteration Management
**As a** convergence engine  
**I want** to stop iterating when converged or stagnated  
**So that** I don't waste tokens on unproductive iterations

**Acceptance Criteria:**
- [ ] Early stop on convergence (score ‚â• threshold)
- [ ] Stop on max iterations (15)
- [ ] Stop on stagnation (no improvement for 3 iterations)
- [ ] Stop on token budget exhaustion

**Stop Conditions:**
```typescript
function shouldStop(state: ConvergenceState, config: ConvergenceConfig): boolean {
  // Early stop (good)
  if (state.score >= config.minScore) return true;
  
  // Max iterations (safety)
  if (state.iterations.length >= config.maxIterations) return true;
  
  // Stagnation (optimization)
  if (hasStagnated(state, config.stagnationLimit)) return true;
  
  // Budget exhaustion (required)
  if (state.tokensUsed >= config.tokenBudget) return true;
  
  return false;
}
```

---

### US-4.4: Timeout & Circuit Breaker
**As a** reliability engineer  
**I want** timeout handling with circuit breakers  
**So that** long-running validations don't block convergence

**Acceptance Criteria:**
- [ ] Async execution with timeout (30s-120s)
- [ ] Circuit breaker after 3 consecutive timeouts
- [ ] Graceful degradation (fallback to LLM validation)
- [ ] Timeout metrics and alerting

**Circuit Breaker Pattern:**
```typescript
export class CircuitBreaker {
  private failures = 0;
  private state: 'CLOSED' | 'OPEN' | 'HALF_OPEN' = 'CLOSED';
  
  async execute<T>(fn: () => Promise<T>): Promise<T> {
    if (this.state === 'OPEN') {
      throw new Error('Circuit breaker OPEN - too many failures');
    }
    
    try {
      const result = await this.withTimeout(fn, 30000);
      this.onSuccess();
      return result;
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }
  
  private onFailure(): void {
    this.failures++;
    if (this.failures >= 3) {
      this.state = 'OPEN';
    }
  }
}
```

---

### US-4.5: Evidence Pack Generation (Hybrid)
**As a** compliance officer  
**I want** deterministic evidence collection + LLM synthesis  
**So that** convergence runs are auditable and human-readable

**Acceptance Criteria:**
- [ ] Code-first evidence collection (deterministic)
- [ ] Cryptographic signatures on evidence (HMAC-SHA256)
- [ ] LLM synthesis for executive summary
- [ ] Evidence pack export (JSON + Markdown)

**Evidence Collection:**
```typescript
// ‚úÖ CODE: Deterministic evidence collection
export async function collectEvidence(
  state: ConvergenceState
): Promise<RawEvidence> {
  return {
    runId: state.runId,
    timestamp: new Date().toISOString(),
    toolInvocations: state.toolCalls.map(formatToolCall),
    validationResults: state.iterations.flatMap(iter => iter.validations),
    artifacts: state.artifacts.map(a => ({
      path: a.path,
      sha256: computeHash(a.content),
      size: a.content.length,
    })),
    metrics: {
      totalIterations: state.iterations.length,
      finalScore: state.score,
      tokensUsed: state.tokensUsed,
      duration: state.endTime - state.startTime,
    },
  };
}

// ‚ùå LLM: Non-deterministic synthesis
export async function synthesizeEvidence(
  raw: RawEvidence
): Promise<EvidenceSynthesis> {
  return await claude.generateReport({
    template: 'evidence-synthesis',
    data: raw,
    sections: ['executiveSummary', 'keyFindings', 'recommendations'],
  });
}
```

---

### US-4.6: Gateway Integration
**As a** MCP gateway  
**I want** to trigger convergence from Answer Contract  
**So that** contract-driven code generation is automated

**Acceptance Criteria:**
- [ ] REST API endpoint: POST /api/v1/converge
- [ ] WebSocket for real-time updates
- [ ] Authentication & authorization
- [ ] Rate limiting (per-user quota)

**API Endpoint:**
```typescript
// POST /api/v1/converge
{
  "contract": "file://contracts/feature-x.yaml",
  "config": {
    "maxIterations": 15,
    "minScore": 0.85,
    "timeout": 60000
  }
}

// Response
{
  "convergenceId": "cvg_abc123",
  "status": "RUNNING",
  "progress": {
    "iteration": 3,
    "score": 0.72,
    "tokensUsed": 8500
  }
}
```

---

### US-4.7: Metrics & Observability
**As a** platform operator  
**I want** convergence metrics and dashboards  
**So that** I can monitor system health and cost

**Acceptance Criteria:**
- [ ] Prometheus metrics export
- [ ] Convergence success rate
- [ ] Token usage metrics
- [ ] Iteration distribution histogram
- [ ] Timeout rate

**Key Metrics:**
```typescript
export const METRICS = {
  convergence_total: new Counter('Total convergence runs'),
  convergence_success_rate: new Gauge('Success rate (%)'),
  convergence_iterations: new Histogram('Iterations per run'),
  convergence_tokens: new Histogram('Tokens per run'),
  convergence_duration: new Histogram('Duration per run (ms)'),
  validator_timeout_rate: new Gauge('Validator timeout rate (%)'),
  token_savings: new Gauge('Token savings vs traditional MCP (%)'),
};
```

---

## Key Deliverables

```
packages/convergence-engine/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ convergence-loop.ts        # Main convergence loop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ state-manager.ts           # Convergence state
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stop-conditions.ts         # When to stop
‚îÇ   ‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validator-executor.ts      # Code-first execution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validator-types.ts         # Validator definitions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ result-aggregator.ts       # Aggregate validation results
‚îÇ   ‚îú‚îÄ‚îÄ iteration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iteration-manager.ts       # Adaptive iteration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stagnation-detector.ts     # Detect no improvement
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ early-stop.ts              # Convergence detection
‚îÇ   ‚îú‚îÄ‚îÄ timeout/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ timeout-manager.ts         # Async timeout handling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ circuit-breaker.ts         # Circuit breaker pattern
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fallback-handler.ts        # Degradation strategy
‚îÇ   ‚îú‚îÄ‚îÄ evidence/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evidence-collector.ts      # Deterministic collection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evidence-synthesizer.ts    # LLM synthesis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evidence-signer.ts         # Cryptographic signatures
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evidence-exporter.ts       # JSON + Markdown export
‚îÇ   ‚îú‚îÄ‚îÄ gateway/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rest-api.ts                # REST endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ websocket-server.ts        # Real-time updates
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth-middleware.ts         # Authentication
‚îÇ   ‚îî‚îÄ‚îÄ metrics/
‚îÇ       ‚îú‚îÄ‚îÄ prometheus-exporter.ts     # Metrics export
‚îÇ       ‚îî‚îÄ‚îÄ dashboard.ts               # Grafana dashboard
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ README.md
```

---

## Completion Criteria

- [ ] All 7 User Stories implemented
- [ ] Convergence loop operational
- [ ] Code-first validators executing (98% token reduction)
- [ ] Adaptive iteration (15 max, early stop, stagnation detection)
- [ ] Timeout handling with circuit breakers
- [ ] Evidence packs generating (code + LLM synthesis)
- [ ] Gateway integration (REST + WebSocket)
- [ ] Metrics exporting to Prometheus
- [ ] All tests passing (>90% coverage)
- [ ] Documentation complete

---

## Success Metrics

**Token Efficiency:**
- Target: **98% token reduction** (150K ‚Üí 2K per convergence)
- Actual: _To be measured_

**Convergence Quality:**
- Target: **95% success rate** (converged within 15 iterations)
- Actual: _To be measured_

**Performance:**
- Target: **<60s per iteration** (generation + validation)
- Actual: _To be measured_

**Reliability:**
- Target: **<5% timeout rate**
- Actual: _To be measured_

---

## Verification Script

```bash
#!/bin/bash
# verify-epic-4.sh

set -e

echo "üîç Verifying Epic 4: Convergence Engine"

# Check package structure
if [ ! -d "packages/convergence-engine" ]; then
  echo "‚ùå convergence-engine package missing"
  exit 1
fi

# Check core files
required_files=(
  "packages/convergence-engine/src/core/convergence-loop.ts"
  "packages/convergence-engine/src/validation/validator-executor.ts"
  "packages/convergence-engine/src/iteration/iteration-manager.ts"
  "packages/convergence-engine/src/timeout/timeout-manager.ts"
  "packages/convergence-engine/src/evidence/evidence-collector.ts"
  "packages/convergence-engine/src/gateway/rest-api.ts"
  "packages/convergence-engine/src/metrics/prometheus-exporter.ts"
  "packages/convergence-engine/package.json"
)

for file in "${required_files[@]}"; do
  if [ ! -f "$file" ]; then
    echo "‚ùå Missing required file: $file"
    exit 1
  fi
done

# Build
echo "üì¶ Building convergence-engine..."
pnpm --filter @forge/convergence-engine build

# Run tests
echo "üß™ Running tests..."
pnpm --filter @forge/convergence-engine test

# Run integration test
echo "üéØ Running convergence integration test..."
pnpm --filter @forge/convergence-engine test:integration

# Check token reduction
echo "üí∞ Checking token reduction..."
token_reduction=$(pnpm --filter @forge/convergence-engine test:token-reduction)
if [ "$token_reduction" -lt 90 ]; then
  echo "‚ùå Token reduction below 90%: ${token_reduction}%"
  exit 1
fi

echo ""
echo "‚úÖ Epic 4 verification complete"
echo "‚úÖ Convergence loop operational"
echo "‚úÖ Token reduction: ${token_reduction}%"
echo "‚úÖ All tests passing"
echo ""
echo "üìã Ready for Epic 5: Multi-Agent Orchestration"
```

---

## ROI Analysis

**Development Investment:**
- Epic 4: 50K tokens √ó $0.02 = $1,000

**Operational Value (Year 1):**
- **Token savings**: 98% reduction = $355,200/year (at 10K convergences/month)
- **Automation**: Eliminate manual code review = 2,000 hours/year √ó $150/hr = $300,000
- **Quality improvement**: 95% success rate = reduce rework by 80% = $100,000/year

**Total Value:** $755,200/year

**Payback Period:** **12 hours** at 10K convergences/month

**ROI:** **75,420%** (conservative, first year only)

---

## Handoff Context for Epic 5

**What Epic 5 needs to know:**

**Convergence Entry Point:**
```typescript
import { ConvergenceEngine } from '@forge/convergence-engine';

const engine = new ConvergenceEngine({
  maxIterations: 15,
  minScore: 0.85,
  timeout: 60000,
});

const result = await engine.converge(contract);
```

**What Epic 5 will add:**
1. Multi-agent orchestration (Initializer + Worker agents)
2. Agent specialization (Frontend, Backend, Testing, Security)
3. Parallel convergence (multiple contracts simultaneously)
4. Agent handoff protocols (context passing)

**Files to reference:**
- `packages/convergence-engine/src/core/convergence-loop.ts` - Convergence algorithm
- `packages/convergence-engine/src/evidence/evidence-collector.ts` - Audit trails
- `packages/convergence-engine/src/metrics/prometheus-exporter.ts` - Observability

---

## References

**Anthropic Guidance:**
1. "Code execution with MCP: building more efficient AI agents" (Nov 2025)
2. "Claude Code: Best practices for agentic coding" (2025)
3. "Effective harnesses for long-running agents" (2025)
4. EPIC-4-CONVERGENCE-QUESTIONS-ANTHROPIC-GUIDANCE.md (This session)

**Production Evidence:**
- GitHub MCP Server: 99%+ token reduction (112 tools)
- Cloudflare "Code Mode" validation
- Anthropic's internal agents: 10-15 iteration limits
